# å¦‚ä½•ä½¿ç”¨Ollama BGE-M3åµŒå…¥æ¨¡å‹

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿï¼‰

### 1. å®‰è£…Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
ä¸‹è½½å®‰è£…åŒ…: https://ollama.ai/download

### 2. å¯åŠ¨OllamaæœåŠ¡

```bash
ollama serve
```

### 3. ä¸‹è½½BGE-M3æ¨¡å‹

```bash
ollama pull bge-m3
```

### 4. é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»ºæˆ–ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š
```env
# RAGåµŒå…¥æ¨¡å‹é…ç½®
EMBEDDING_MODEL_TYPE=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=bge-m3
```

### 5. é‡å»ºRAGç´¢å¼•

```bash
cd /Users/zelaszang/Documents/UGit/content-moderation-system

# åˆ é™¤æ—§ç´¢å¼•
rm -rf data/vector_store

# é‡å»ºç´¢å¼•
pipenv run python scripts/build_rag_index.py
```

### 6. è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•Ollamaè¿æ¥
pipenv run python -m pytest tests/test_ollama_embedding.py -v -s

# æµ‹è¯•RAGé›†æˆ
pipenv run python -m pytest tests/test_task_2_4.py -v -s
```

---

## ğŸ”„ åˆ‡æ¢åµŒå…¥æ¨¡å‹

### ä½¿ç”¨Ollamaï¼ˆæ¨èï¼‰

```bash
export EMBEDDING_MODEL_TYPE=ollama
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_EMBEDDING_MODEL=bge-m3
```

### ä½¿ç”¨OpenAI

```bash
export EMBEDDING_MODEL_TYPE=openai
export OPENAI_API_KEY=your_api_key
```

**æ³¨æ„**: åˆ‡æ¢æ¨¡å‹åéœ€è¦é‡å»ºç´¢å¼•ï¼

---

## âœ… éªŒè¯å®‰è£…

### æ£€æŸ¥OllamaæœåŠ¡
```bash
curl http://localhost:11434/api/tags
```

### æ£€æŸ¥BGE-M3æ¨¡å‹
```bash
ollama list | grep bge-m3
```

### è¿è¡Œå¿«é€Ÿæµ‹è¯•
```bash
pipenv run python -c "
from llama_index.embeddings.ollama import OllamaEmbedding
embed = OllamaEmbedding(model_name='bge-m3', base_url='http://localhost:11434')
result = embed.get_text_embedding('æµ‹è¯•æ–‡æœ¬')
print(f'âœ… åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(result)}')
"
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | OpenAI | Ollama BGE-M3 |
|-----|--------|---------------|
| æˆæœ¬ | $0.02/1M tokens | å…è´¹ |
| å»¶è¿Ÿ | ~200ms | ~50ms |
| æ•°æ®å®‰å…¨ | äº‘ç«¯ | æœ¬åœ° |
| å‘é‡ç»´åº¦ | 1536 | 1024 |

---

## â“ å¸¸è§é—®é¢˜

### Q1: OllamaæœåŠ¡æ— æ³•å¯åŠ¨ï¼Ÿ
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434

# é‡å¯æœåŠ¡
pkill ollama
ollama serve
```

### Q2: æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Ÿ
```bash
# ä½¿ç”¨ä»£ç†
export https_proxy=http://127.0.0.1:7890
ollama pull bge-m3
```

### Q3: åµŒå…¥ç”Ÿæˆå¾ˆæ…¢ï¼Ÿ
- æ£€æŸ¥CPUä½¿ç”¨ç‡
- è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿ
- ä½¿ç”¨æ‰¹é‡åµŒå…¥

### Q4: å¦‚ä½•å›é€€åˆ°OpenAIï¼Ÿ
```bash
# ä¿®æ”¹.env
EMBEDDING_MODEL_TYPE=openai
OPENAI_API_KEY=your_key

# é‡å»ºç´¢å¼•
rm -rf data/vector_store
pipenv run python scripts/build_rag_index.py
```

---

## ğŸ“š æ›´å¤šä¿¡æ¯

- [Ollamaå®˜æ–¹æ–‡æ¡£](https://ollama.ai/docs)
- [BGE-M3æ¨¡å‹ä»‹ç»](https://github.com/FlagOpen/FlagEmbedding)
- [LlamaIndex Ollamaé›†æˆ](https://docs.llamaindex.ai/en/stable/examples/embeddings/ollama_embedding/)
- [å®Œæ•´æµ‹è¯•æŠ¥å‘Š](./Ollamaé›†æˆæµ‹è¯•æŠ¥å‘Š.md)

---

## ğŸ’¡ æç¤º

- âœ… é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆ~1.2GBï¼‰
- âœ… ç¡®ä¿OllamaæœåŠ¡åœ¨åå°è¿è¡Œ
- âœ… åˆ‡æ¢æ¨¡å‹åå¿…é¡»é‡å»ºç´¢å¼•
- âœ… å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨Ollamaé™ä½æˆæœ¬
